{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c23085d-fcfd-4732-a151-34509740a832",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+----------------+----------+----------+-------------+----------+--------------------+------+\n| id|     customername|           state|      city|created_on|date_of_birth|updated_on|               email|gender|\n+---+-----------------+----------------+----------+----------+-------------+----------+--------------------+------+\n|267|      Mala Pratap|  Madhya Pradesh|    Indore|2018-12-06|   1983-11-04|2018-12-06|Mala Pratap@outlo...|     f|\n| 59|          Anudeep|  Madhya Pradesh|    Indore|2018-08-26|   1978-09-09|2018-08-26|                null|  null|\n|273|    Shakshi Sagar|        Nagaland|    Kohima|2018-04-17|   1996-11-06|2019-03-27|     Sagar@gmail.com|     f|\n|116|     Ekta Chauhan|  Madhya Pradesh|    Indore|2018-06-28|   1987-04-20|2018-06-28|Ekta Chauhan@outl...|     f|\n| 92|         Bhutekar|  Madhya Pradesh|    Indore|2019-01-04|   1989-10-08|2019-01-04|                null|  null|\n| 48|    Anjali Juneja|           Delhi|     Delhi|2019-02-01|   1987-11-24|2019-02-01|    Anjali@gmail.com|     f|\n| 49|    Anjali Juneja|         Haryana|Chandigarh|2018-05-23|   1979-05-28|2018-05-23|    Anjali@gmail.com|     f|\n|266|       Mala Bagga|Himachal Pradesh|     Simla|2018-11-03|   1999-05-07|2018-11-03|     Rohan@gmail.com|     f|\n|113|          Divyeta|  Madhya Pradesh|    Indore|2018-11-23|   1971-02-19|2018-11-23|                null|  null|\n|122|            Ginny|  Madhya Pradesh|    Indore|2019-01-31|   1992-10-07|2019-01-31|                null|  null|\n|135|           Hitika|  Madhya Pradesh|    Indore|2019-02-23|   1987-12-16|2019-03-31|                null|  null|\n| 50|    Anjali Juneja|     Maharashtra|    Mumbai|2018-07-12|   1997-01-31|2018-07-12|    Anjali@gmail.com|     f|\n|157|       Kiran Devi|     Maharashtra|    Mumbai|2018-07-21|   1996-02-19|2018-07-21|     Kiran@gmail.com|     f|\n|159|          Kishwar|  Madhya Pradesh|    Indore|2018-07-01|   1970-09-27|2018-07-01|                null|  null|\n|167|          Kshitij|  Madhya Pradesh|    Indore|2018-10-10|   1974-11-30|2018-10-10|                null|  null|\n|268|        Mala Garg|     Maharashtra|    Mumbai|2018-10-18|   1973-12-17|2018-10-18|     Rohan@gmail.com|     f|\n|174|        Maithilee|  Madhya Pradesh|    Indore|2018-06-26|   1976-07-28|2018-06-26|                null|  null|\n|188|           Mhatre|  Madhya Pradesh|    Indore|2019-01-11|   1980-01-16|2019-01-11|                null|  null|\n|125|     Gunjan Verna|  Madhya Pradesh|    Indore|2018-06-15|   1976-07-17|2018-06-15|Gunjan Verna@outl...|     f|\n|331|Smt. Shweta Gupta|  Madhya Pradesh|    Indore|2018-06-30|   1987-04-22|2018-06-30|Smt. Shweta Gupta...|     f|\n+---+-----------------+----------------+----------+----------+-------------+----------+--------------------+------+\nonly showing top 20 rows\n\n+--------+------+------+--------+----------+\n|order_id|amount|profit|quantity|product_id|\n+--------+------+------+--------+----------+\n| B-26010|  18.0|   2.0|       3|         1|\n| B-25667|  11.0|  -2.0|       4|         1|\n| B-26016| 202.0|   4.0|       4|         1|\n| B-26018|  61.0|   8.0|       4|         1|\n| B-26021|  21.0| -12.0|       3|         1|\n| B-26025|  41.0|  19.0|       5|         1|\n| B-26026| 255.0|  76.0|       9|         1|\n| B-26030|  92.0|   5.0|       6|         1|\n| B-26097|  19.0|   8.0|       2|         1|\n| B-25670|  24.0|   1.0|       2|         1|\n| B-25670|  14.0|   2.0|       1|         1|\n| B-25811| 126.0|  52.0|       4|         1|\n| B-25818|  75.0|  28.0|       9|         1|\n| B-25810|  29.0|   8.0|       5|         1|\n| B-25810|  26.0|  10.0|       4|         1|\n| B-25809|  53.0|  24.0|       1|         1|\n| B-25809| 154.0|  54.0|       3|         1|\n| B-25808| 210.0|  50.0|       4|         1|\n| B-25870| 473.0| 113.0|       9|         1|\n| B-25871| 118.0|  25.0|       4|         1|\n+--------+------+------+--------+----------+\nonly showing top 20 rows\n\n+-------+----------+-----------+\n|     id|order_date|customer_id|\n+-------+----------+-----------+\n|B-25709|2018-07-01|          1|\n|B-26081|2019-03-22|          2|\n|B-26018|2019-02-14|          2|\n|B-25608|2018-04-08|          2|\n|B-25893|2018-12-04|          3|\n|B-25830|2018-10-26|          4|\n|B-25977|2019-01-27|          6|\n|B-25680|2018-06-04|          7|\n|B-25782|2018-09-15|          8|\n|B-25861|2018-11-15|          5|\n|B-25855|2018-11-08|          9|\n|B-25856|2018-11-10|         10|\n|B-25906|2018-12-11|         11|\n|B-25850|2018-11-05|         12|\n|B-25889|2018-12-03|         13|\n|B-25786|2018-09-19|         14|\n|B-25792|2018-09-24|         15|\n|B-25898|2018-12-07|         16|\n|B-25712|2018-07-07|         17|\n|B-25777|2018-09-10|         18|\n+-------+----------+-----------+\nonly showing top 20 rows\n\n+---+-----------+----------------+\n| id|   category|         product|\n+---+-----------+----------------+\n|  1|   Clothing|     Hankerchief|\n|  2|   Clothing|           Kurti|\n|  3|   Clothing|        Leggings|\n|  4|   Clothing|           Saree|\n|  5|   Clothing|           Shirt|\n|  6|   Clothing|           Skirt|\n|  7|   Clothing|           Stole|\n|  8|   Clothing|         T-shirt|\n|  9|   Clothing|        Trousers|\n| 10|Electronics|     Accessories|\n| 11|Electronics|Electronic Games|\n| 12|Electronics|          Phones|\n| 13|Electronics|        Printers|\n| 14|  Furniture|       Bookcases|\n| 15|  Furniture|          Chairs|\n| 16|  Furniture|     Furnishings|\n| 17|  Furniture|          Tables|\n+---+-----------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# File paths for CSV files\n",
    "customers_file_path = \"dbfs:/FileStore/shared_uploads/riakundra2003@gmail.com/customers.csv\"\n",
    "order_items_file_path = \"dbfs:/FileStore/shared_uploads/riakundra2003@gmail.com/order_items.csv\"\n",
    "orders_file_path = \"dbfs:/FileStore/shared_uploads/riakundra2003@gmail.com/orders.csv\"\n",
    "product_file_path = \"dbfs:/FileStore/shared_uploads/riakundra2003@gmail.com/product.csv\"\n",
    "\n",
    "# Load CSV files into DataFrames\n",
    "customers_df = spark.read.csv(customers_file_path, header=True, inferSchema=True)\n",
    "order_items_df = spark.read.csv(order_items_file_path, header=True, inferSchema=True)\n",
    "orders_df = spark.read.csv(orders_file_path, header=True, inferSchema=True)\n",
    "product_df = spark.read.csv(product_file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Display first few rows to ensure proper load\n",
    "customers_df.show()\n",
    "order_items_df.show()\n",
    "orders_df.show()\n",
    "product_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "464ac05e-6ed1-40fe-a5ba-785ab4aa6e59",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define paths for Bronze Delta tables\n",
    "bronze_customers_path = \"/bronze/customers\"\n",
    "bronze_order_items_path = \"/bronze/order_items\"\n",
    "bronze_orders_path = \"/bronze/orders\"\n",
    "bronze_product_path = \"/bronze/product\"\n",
    "\n",
    "# Save DataFrames to Delta tables in the Bronze layer\n",
    "customers_df.write.format(\"delta\").mode(\"overwrite\").save(bronze_customers_path)\n",
    "order_items_df.write.format(\"delta\").mode(\"overwrite\").save(bronze_order_items_path)\n",
    "orders_df.write.format(\"delta\").mode(\"overwrite\").save(bronze_orders_path)\n",
    "product_df.write.format(\"delta\").mode(\"overwrite\").save(bronze_product_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7263fb96-7140-49e7-a907-0f8f29853204",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----+----+----------+-------------+----------+-----+------+\n| id|customername|state|city|created_on|date_of_birth|updated_on|email|gender|\n+---+------------+-----+----+----------+-------------+----------+-----+------+\n|  0|           0|    0|   0|         0|            0|         0|   20|   158|\n+---+------------+-----+----+----------+-------------+----------+-----+------+\n\n+--------+------+------+--------+----------+\n|order_id|amount|profit|quantity|product_id|\n+--------+------+------+--------+----------+\n|       0|     0|     0|       0|         0|\n+--------+------+------+--------+----------+\n\n+---+----------+-----------+\n| id|order_date|customer_id|\n+---+----------+-----------+\n|  0|         0|          0|\n+---+----------+-----------+\n\n+---+--------+-------+\n| id|category|product|\n+---+--------+-------+\n|  0|       0|      0|\n+---+--------+-------+\n\nDuplicate customers: 0\nDuplicate order items: 17\nDuplicate orders: 0\nDuplicate products: 0\n"
     ]
    }
   ],
   "source": [
    "# Importing functions module from PySpark\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Checking for null values in individual columns\n",
    "customers_null_count = customers_df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in customers_df.columns])\n",
    "order_items_null_count = order_items_df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in order_items_df.columns])\n",
    "orders_null_count = orders_df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in orders_df.columns])\n",
    "product_null_count = product_df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in product_df.columns])\n",
    "\n",
    "# Show the null count for each DataFrame\n",
    "customers_null_count.show()\n",
    "order_items_null_count.show()\n",
    "orders_null_count.show()\n",
    "product_null_count.show()\n",
    "\n",
    "# Checking for duplicates\n",
    "# Checking for duplicates - replace 'order_id' with the correct column name 'id'\n",
    "customers_dup_count = customers_df.groupBy(\"id\").count().filter(\"count > 1\").count()\n",
    "\n",
    "# Correcting for `order_items_df` (use 'product_id' or combination as needed)\n",
    "order_items_dup_count = order_items_df.groupBy(\"product_id\").count().filter(\"count > 1\").count()\n",
    "\n",
    "# Check for duplicates in the orders DataFrame (use 'id' instead of 'order_id')\n",
    "orders_dup_count = orders_df.groupBy(\"id\").count().filter(\"count > 1\").count()\n",
    "\n",
    "# Check for duplicates in the products DataFrame\n",
    "product_dup_count = product_df.groupBy(\"id\").count().filter(\"count > 1\").count()\n",
    "\n",
    "# Display the counts of duplicates\n",
    "print(f\"Duplicate customers: {customers_dup_count}\")\n",
    "print(f\"Duplicate order items: {order_items_dup_count}\")\n",
    "print(f\"Duplicate orders: {orders_dup_count}\")\n",
    "print(f\"Duplicate products: {product_dup_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6de7869-c2a0-443a-b830-cc7b26529f80",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Data Cleaning\n",
    "\n",
    "# Example cleaning: Remove rows with null values in critical columns\n",
    "customers_cleaned = customers_df.na.drop(subset=[\"id\", \"customername\", \"email\"])\n",
    "orders_cleaned = orders_df.na.drop(subset=[\"id\", \"customer_id\"])\n",
    "order_items_cleaned = order_items_df.na.drop(subset=[\"order_id\", \"product_id\", \"quantity\"])\n",
    "product_cleaned = product_df.na.drop(subset=[\"id\", \"product\"])\n",
    "\n",
    "# Step 2: Join Tables\n",
    "\n",
    "# Join orders with customers to get customer details for each order\n",
    "orders_with_customers = orders_cleaned.join(\n",
    "    customers_cleaned,\n",
    "    orders_cleaned.customer_id == customers_cleaned.id,\n",
    "    \"left\"\n",
    ").select(orders_cleaned.id.alias(\"order_id\"), \"customer_id\", \"order_date\", \"customername\", \"email\", \"state\", \"city\")\n",
    "\n",
    "\n",
    "# Join the result with order items to get item details for each order\n",
    "orders_with_items = orders_with_customers.join(\n",
    "    order_items_cleaned,\n",
    "    orders_with_customers.order_id == order_items_cleaned.order_id,\n",
    "    \"left\"\n",
    ").select(orders_with_customers.order_id, \"customer_id\", \"order_date\", \"customername\", \"email\", \n",
    "           order_items_cleaned.product_id, \"quantity\", \"profit\", \"amount\",\"state\", \"city\")\n",
    "\n",
    "\n",
    "# Join with products to get product details\n",
    "final_silver_df = orders_with_items.join(\n",
    "    product_cleaned,\n",
    "    orders_with_items.product_id == product_cleaned.id,\n",
    "    \"left\"\n",
    ").select(orders_with_items.order_id, orders_with_items.customer_id, \"order_date\", \"customername\", \"email\", \n",
    "           product_cleaned.product.alias(\"product_name\"), \"category\", \n",
    "           \"quantity\", \"profit\", \"amount\", \"state\", \"city\")\n",
    "\n",
    "\n",
    "silver_table_path = \"/silver/transformed_orders13\"\n",
    "\n",
    "# Write the DataFrame to Delta format\n",
    "final_silver_df.write.format(\"delta\").mode(\"overwrite\").save(silver_table_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "273fb0bd-214f-4379-96ec-ec9901faf2f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+-------------+--------------------+------------+-----------+--------+-------+------+--------------+---------+\n|order_id|customer_id|order_date| customername|               email|product_name|   category|quantity| profit|amount|         state|     city|\n+--------+-----------+----------+-------------+--------------------+------------+-----------+--------+-------+------+--------------+---------+\n| B-25709|          1|2018-07-01|Aakanksha D/O|Aakanksha D/O@out...|      Chairs|  Furniture|       1|   -6.0|  41.0|Madhya Pradesh|   Indore|\n| B-25709|          1|2018-07-01|Aakanksha D/O|Aakanksha D/O@out...|       Saree|   Clothing|       7|  -12.0|  33.0|Madhya Pradesh|   Indore|\n| B-26081|          2|2019-03-22|      Aarushi|   Aarushi@gmail.com|   Bookcases|  Furniture|       5| -338.0| 359.0|    Tamil Nadu|  Chennai|\n| B-26081|          2|2019-03-22|      Aarushi|   Aarushi@gmail.com| Accessories|Electronics|       3|    0.0| 169.0|    Tamil Nadu|  Chennai|\n| B-26081|          2|2019-03-22|      Aarushi|   Aarushi@gmail.com|       Stole|   Clothing|       4|   33.0|  79.0|    Tamil Nadu|  Chennai|\n| B-26081|          2|2019-03-22|      Aarushi|   Aarushi@gmail.com|       Saree|   Clothing|       5|   50.0| 637.0|    Tamil Nadu|  Chennai|\n| B-26081|          2|2019-03-22|      Aarushi|   Aarushi@gmail.com|       Saree|   Clothing|       3|  -84.0|  93.0|    Tamil Nadu|  Chennai|\n| B-26081|          2|2019-03-22|      Aarushi|   Aarushi@gmail.com| Hankerchief|   Clothing|       3|   11.0|  24.0|    Tamil Nadu|  Chennai|\n| B-26018|          2|2019-02-14|      Aarushi|   Aarushi@gmail.com| Furnishings|  Furniture|       3|  107.0| 326.0|    Tamil Nadu|  Chennai|\n| B-26018|          2|2019-02-14|      Aarushi|   Aarushi@gmail.com| Hankerchief|   Clothing|       4|    8.0|  61.0|    Tamil Nadu|  Chennai|\n| B-25608|          2|2018-04-08|      Aarushi|   Aarushi@gmail.com|      Tables|  Furniture|       5|-1864.0|1364.0|    Tamil Nadu|  Chennai|\n| B-25608|          2|2018-04-08|      Aarushi|   Aarushi@gmail.com|      Chairs|  Furniture|       3|    0.0| 476.0|    Tamil Nadu|  Chennai|\n| B-25608|          2|2018-04-08|      Aarushi|   Aarushi@gmail.com|    Printers|Electronics|       6|  385.0| 856.0|    Tamil Nadu|  Chennai|\n| B-25608|          2|2018-04-08|      Aarushi|   Aarushi@gmail.com| Hankerchief|   Clothing|       5|   23.0| 257.0|    Tamil Nadu|  Chennai|\n| B-25893|          3|2018-12-04|       Aashna|    Aashna@gmail.com|    Printers|Electronics|       3|   59.0| 372.0| Uttar Pradesh|Allahabad|\n| B-25893|          3|2018-12-04|       Aashna|    Aashna@gmail.com| Accessories|Electronics|       6|  103.0| 688.0| Uttar Pradesh|Allahabad|\n| B-25893|          3|2018-12-04|       Aashna|    Aashna@gmail.com|       Stole|   Clothing|       3|   12.0|  83.0| Uttar Pradesh|Allahabad|\n| B-25893|          3|2018-12-04|       Aashna|    Aashna@gmail.com|       Stole|   Clothing|       7|  114.0| 355.0| Uttar Pradesh|Allahabad|\n| B-25893|          3|2018-12-04|       Aashna|    Aashna@gmail.com|       Shirt|   Clothing|       7|   62.0| 223.0| Uttar Pradesh|Allahabad|\n| B-25893|          3|2018-12-04|       Aashna|    Aashna@gmail.com|       Saree|   Clothing|       3|   15.0| 149.0| Uttar Pradesh|Allahabad|\n+--------+-----------+----------+-------------+--------------------+------------+-----------+--------+-------+------+--------------+---------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "final_silver_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "923679e6-fe36-4a12-b6fc-29557d6d4e15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- order_id: string (nullable = true)\n |-- customer_id: integer (nullable = true)\n |-- order_date: date (nullable = true)\n |-- customername: string (nullable = true)\n |-- email: string (nullable = true)\n |-- product_name: string (nullable = true)\n |-- category: string (nullable = true)\n |-- quantity: integer (nullable = true)\n |-- profit: double (nullable = true)\n |-- amount: double (nullable = true)\n |-- state: string (nullable = true)\n |-- city: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Read Data from Silver Table\n",
    "silver_table_path = \"/silver/transformed_orders13\"\n",
    "silver_df = spark.read.format(\"delta\").load(silver_table_path)\n",
    "# Display the schema of the DataFrame\n",
    "silver_df.printSchema()\n",
    "\n",
    "#create a database for gold layer\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS gold_db3\")\n",
    "\n",
    "# Step 2: Perform  required Aggregations and save in the presentation layer\n",
    "\n",
    "# 1 Total profit by product category\n",
    "total_profit_by_category = silver_df.groupBy(\"category\").agg(\n",
    "    F.sum(\"profit\").alias(\"total_profit\")\n",
    ").orderBy(\"category\")\n",
    "\n",
    "total_profit_by_category.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold_db3.total_profit_by_category\")\n",
    "\n",
    "# 2 Number of orders placed by customers in each city instead of state\n",
    "orders_by_state = final_silver_df.groupBy(\"state\").agg(\n",
    "    F.countDistinct(\"order_id\").alias(\"num_orders\")\n",
    ")\n",
    "orders_by_state = orders_by_state.na.drop(subset=[\"state\"])\n",
    "\n",
    "orders_by_state.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold_db3.orders_by_state\")\n",
    "\n",
    "# 3 Total amount spent by each customer\n",
    "total_spent_by_customer = silver_df.groupBy(\"customer_id\").agg(\n",
    "    F.sum(\"amount\").alias(\"total_spent\")\n",
    ").orderBy(\"customer_id\")\n",
    "\n",
    "total_spent_by_customer.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold_db3.total_spent_by_customer\")\n",
    "\n",
    "#df = spark.read.format(\"delta\").load(\"/gold/total_spent_by_customer\")\n",
    "#print(df.dtypes)\n",
    "#print(total_spent_by_customer.dtypes)\n",
    "\n",
    "# 4 Average profit per order for each city\n",
    "average_profit_per_order_by_city = final_silver_df.groupBy(\"city\").agg(\n",
    "    (F.sum(\"profit\") / F.countDistinct(\"order_id\")).alias(\"average_profit_per_order\")\n",
    ")\n",
    "\n",
    "average_profit_per_order_by_city = average_profit_per_order_by_city.na.drop(subset=[\"city\"])\n",
    "\n",
    "average_profit_per_order_by_city.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold_db3.average_profit_per_order_by_city\")\n",
    "\n",
    "# 5 Top 5 customers who spent the most\n",
    "top_customers = silver_df.groupBy(\"customer_id\").agg(\n",
    "    F.sum(\"amount\").alias(\"total_spent\")\n",
    ").orderBy(F.desc(\"total_spent\")).limit(5)\n",
    "\n",
    "top_customers.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold_db3.top_customers\")\n",
    "\n",
    "# 6 Total revenue generated by each product\n",
    "total_revenue_by_product = silver_df.groupBy(\"product_name\").agg(\n",
    "    F.sum(\"amount\").alias(\"total_revenue\")\n",
    ").orderBy(\"product_name\")\n",
    "\n",
    "total_revenue_by_product.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold_db3.total_revenue_by_product\")\n",
    "\n",
    "# 7 Number of orders placed in each product category\n",
    "orders_by_product_category = silver_df.groupBy(\"category\").agg(\n",
    "    F.countDistinct(\"order_id\").alias(\"order_count\")\n",
    ").orderBy(\"category\")\n",
    "\n",
    "orders_by_product_category.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold_db3.orders_by_product_category\")\n",
    "\n",
    "# 8 Average profit per order for each product category\n",
    "average_profit_per_order_by_category = silver_df.groupBy(\"category\").agg(\n",
    "    F.avg(\"profit\").alias(\"average_profit_per_order\")\n",
    ").orderBy(\"category\")\n",
    "\n",
    "average_profit_per_order_by_category.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold_db3.average_profit_per_order_by_category\")\n",
    "\n",
    "# 9 Total amount spent in each city\n",
    "total_amount_by_city = final_silver_df.groupBy(\"city\").agg(\n",
    "    F.sum(\"amount\").alias(\"total_amount_spent\")\n",
    ")\n",
    "\n",
    "total_amount_by_city = total_amount_by_city.na.drop(subset=[\"city\"])\n",
    "\n",
    "total_amount_by_city.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold_db3.total_amount_by_city\")\n",
    "\n",
    "# 10 Number of orders placed for each product\n",
    "orders_by_product = silver_df.groupBy(\"product_name\").agg(\n",
    "    F.countDistinct(\"order_id\").alias(\"order_count\")\n",
    ").orderBy(\"product_name\")\n",
    "\n",
    "orders_by_product.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold_db3.orders_by_product\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1aa55ebd-0cb9-47e1-9c1a-71d4b6da2f1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n|customer_id|total_spent|\n+-----------+-----------+\n|        397|     9177.0|\n|        360|     6611.0|\n|        343|     6339.0|\n|        129|     6026.0|\n|        239|     5809.0|\n+-----------+-----------+\n\nOut[13]: [('customer_id', 'int'), ('total_spent', 'double')]"
     ]
    }
   ],
   "source": [
    "top_customers.show()\n",
    "top_customers.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ae364f4-2ba8-49c2-bb04-c0ecb17d47c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+\n|   category|total_profit|\n+-----------+------------+\n|   Clothing|     11163.0|\n|Electronics|     10494.0|\n|  Furniture|      2298.0|\n+-----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "total_profit_by_category.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57147a67-f909-47b1-b332-d14fd4429d96",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Project",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
